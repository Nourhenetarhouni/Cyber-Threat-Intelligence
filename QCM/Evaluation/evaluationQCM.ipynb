{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwdUGX2lfwjh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def convert_tsv_to_csv(tsv_file, csv_file):\n",
        "    # Lire le fichier TSV\n",
        "    df = pd.read_csv(tsv_file, sep='\\t', encoding='utf-8')\n",
        "\n",
        "    # Assurez-vous que vous avez les bonnes colonnes\n",
        "    # Exemple : 'Question', 'Option A', 'Option B', 'Option C', 'Option D', 'Correct Answer', 'Explanation'\n",
        "    # Vous aurez probablement besoin d'adapter les noms des colonnes selon votre fichier TSV\n",
        "\n",
        "    # Créer un DataFrame pour l'évaluation\n",
        "    evaluation_df = pd.DataFrame({\n",
        "        'Prediction': df['Correct Answer'],  # Remplacez cette colonne par la colonne des prédictions si vous avez une colonne pour les prédictions\n",
        "        'GT': df['Correct Answer']  # Réponses correctes\n",
        "    })\n",
        "\n",
        "    # Enregistrer le DataFrame en tant que fichier CSV\n",
        "    evaluation_df.to_csv(csv_file, index=False)\n",
        "\n",
        "# Chemins vers vos fichiers\n",
        "tsv_file = 'qcm_output1.tsv'  # Remplacez par le chemin de votre fichier TSV\n",
        "csv_file = 'evalqcm.csv'  # Nom du fichier CSV à créer\n",
        "\n",
        "convert_tsv_to_csv(tsv_file, csv_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle"
      ],
      "metadata": {
        "id": "Av4DFulWhg7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compute_mcq_accuracy(fname, col):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(fname, encoding=\"latin1\")\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over each row in the DataFrame\n",
        "    for idx, row in df.iterrows():\n",
        "        # Get the prediction and ground truth, and clean up any possible leading/trailing spaces\n",
        "        pred = str(row[col]).strip().upper()\n",
        "        gt = str(row['GT']).strip().upper()\n",
        "\n",
        "        # Debugging: Print the values being compared\n",
        "\n",
        "\n",
        "        # Check if the prediction is valid (A, B, C, D, X)\n",
        "        if pred in ['A', 'B', 'C', 'D', 'X']:\n",
        "            total += 1\n",
        "            # Check if the prediction matches the ground truth\n",
        "            if pred == gt:\n",
        "                correct += 1\n",
        "        else:\n",
        "            print(f'Invalid response at row {idx+1}: {pred}')\n",
        "\n",
        "    # Avoid division by zero in case there are no valid responses\n",
        "    if total == 0:\n",
        "        print(\"No valid responses found.\")\n",
        "        return 0\n",
        "\n",
        "    # Debugging: Print the final counts\n",
        "    print(f\"Total Valid Responses: {total}, Correct Responses: {correct}\")\n",
        "\n",
        "    # Return the accuracy as a percentage\n",
        "    return (correct / total) * 100\n"
      ],
      "metadata": {
        "id": "bR01QDlYhm27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def print_csv_columns(csv_file):\n",
        "    # Lire le fichier CSV\n",
        "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
        "\n",
        "    # Imprimer les colonnes disponibles\n",
        "    print('Colonnes disponibles dans le fichier CSV :', df.columns.tolist())\n",
        "\n",
        "# Chemin vers votre fichier CSV\n",
        "csv_file = '/content/evalqcm.csv'\n",
        "\n",
        "print_csv_columns(csv_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCEBvWtKhqG2",
        "outputId": "4e117a88-4634-44ac-808f-0f86aa063e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colonnes disponibles dans le fichier CSV : ['Prediction', 'GT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compute_mcq_accuracy(fname, pred_col, gt_col):\n",
        "    # Lire le fichier CSV\n",
        "    df = pd.read_csv(fname, encoding='latin1')\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Vérifiez que les colonnes existent\n",
        "    if pred_col not in df.columns or gt_col not in df.columns:\n",
        "        raise ValueError(f\"Colonnes {pred_col} ou {gt_col} non trouvées dans le fichier CSV.\")\n",
        "\n",
        "    # Itérer sur chaque ligne du DataFrame\n",
        "    for idx, row in df.iterrows():\n",
        "        # Obtenir la prédiction et la vérité de terrain, et nettoyer les espaces éventuels\n",
        "        pred = str(row[pred_col]).strip().upper()\n",
        "        gt = str(row[gt_col]).strip().upper()\n",
        "\n",
        "        # Debugging : Print the values being compared\n",
        "        # print(f\"Row {idx + 1} - Prediction: {pred}, Ground Truth: {gt}\")\n",
        "\n",
        "        # Vérifier si la prédiction est une seule réponse valide (A, B, C, D)\n",
        "        if pred in ['A', 'B', 'C', 'D'] and gt in ['A', 'B', 'C', 'D']:\n",
        "            total += 1\n",
        "            # Vérifiez si la prédiction correspond à la vérité de terrain\n",
        "            if pred == gt:\n",
        "                correct += 1\n",
        "        else:\n",
        "            print(f'Réponse invalide à la ligne {idx + 1}: {pred}')\n",
        "\n",
        "    # Éviter la division par zéro en cas d'aucune réponse valide\n",
        "    if total == 0:\n",
        "        print(\"Aucune réponse valide trouvée.\")\n",
        "        return 0\n",
        "\n",
        "    # Imprimer les comptes finaux\n",
        "    print(f\"Total des réponses valides : {total}, Réponses correctes : {correct}\")\n",
        "\n",
        "    # Retourner la précision en pourcentage\n",
        "    return (correct / total) * 100\n",
        "\n",
        "# Utiliser le script\n",
        "print('Précision:', compute_mcq_accuracy('/content/evalqcm.csv', 'Prediction', 'GT'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImqHspX2iNPn",
        "outputId": "7a1be226-1142-489d-ee42-1efa7dd5b1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Réponse invalide à la ligne 24: NAN\n",
            "Réponse invalide à la ligne 25: NAN\n",
            "Réponse invalide à la ligne 42: A, B, D\n",
            "Réponse invalide à la ligne 43: NAN\n",
            "Réponse invalide à la ligne 45: A, B\n",
            "Réponse invalide à la ligne 52: NAN\n",
            "Réponse invalide à la ligne 53: NAN\n",
            "Réponse invalide à la ligne 54: NAN\n",
            "Réponse invalide à la ligne 55: NAN\n",
            "Réponse invalide à la ligne 56: NAN\n",
            "Réponse invalide à la ligne 90: NAN\n",
            "Réponse invalide à la ligne 130: NAN\n",
            "Réponse invalide à la ligne 131: NAN\n",
            "Réponse invalide à la ligne 148: A, B, D\n",
            "Réponse invalide à la ligne 149: NAN\n",
            "Réponse invalide à la ligne 151: A, B\n",
            "Réponse invalide à la ligne 158: NAN\n",
            "Réponse invalide à la ligne 159: NAN\n",
            "Réponse invalide à la ligne 160: NAN\n",
            "Réponse invalide à la ligne 161: NAN\n",
            "Réponse invalide à la ligne 162: NAN\n",
            "Réponse invalide à la ligne 196: NAN\n",
            "Réponse invalide à la ligne 236: NAN\n",
            "Réponse invalide à la ligne 237: NAN\n",
            "Réponse invalide à la ligne 254: A, B, D\n",
            "Réponse invalide à la ligne 255: NAN\n",
            "Réponse invalide à la ligne 257: A, B\n",
            "Réponse invalide à la ligne 264: NAN\n",
            "Réponse invalide à la ligne 265: NAN\n",
            "Réponse invalide à la ligne 266: NAN\n",
            "Réponse invalide à la ligne 267: NAN\n",
            "Réponse invalide à la ligne 268: NAN\n",
            "Réponse invalide à la ligne 302: NAN\n",
            "Total des réponses valides : 285, Réponses correctes : 285\n",
            "Précision: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compare_datasets(file1, file2, pred_col, gt_col):\n",
        "    # Lire les deux fichiers TSV\n",
        "    df1 = pd.read_csv(file1, sep='\\t', encoding='utf-8')\n",
        "    df2 = pd.read_csv(file2, sep='\\t', encoding='utf-8')\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Vérifiez que les colonnes existent\n",
        "    if pred_col not in df1.columns or gt_col not in df2.columns:\n",
        "        raise ValueError(f\"Colonnes {pred_col} ou {gt_col} non trouvées dans les fichiers.\")\n",
        "\n",
        "    # Itérer sur chaque ligne des deux DataFrames en parallèle\n",
        "    for idx, (row1, row2) in enumerate(zip(df1.iterrows(), df2.iterrows())):\n",
        "        pred = str(row1[1][pred_col]).strip().upper()  # Colonne de prédiction dans le premier fichier\n",
        "        gt = str(row2[1][gt_col]).strip().upper()  # Colonne de vérité de terrain dans le deuxième fichier\n",
        "\n",
        "        # Vérifier si la prédiction est une seule réponse valide (A, B, C, D)\n",
        "        if pred in ['A', 'B', 'C', 'D'] and gt in ['A', 'B', 'C', 'D']:\n",
        "            total += 1\n",
        "            # Vérifier si la prédiction correspond à la vérité de terrain\n",
        "            if pred == gt:\n",
        "                correct += 1\n",
        "        else:\n",
        "            print(f'Réponse invalide à la ligne {idx + 1}: {pred}')\n",
        "\n",
        "    # Éviter la division par zéro en cas d'aucune réponse valide\n",
        "    if total == 0:\n",
        "        print(\"Aucune réponse valide trouvée.\")\n",
        "        return 0\n",
        "\n",
        "    # Imprimer les comptes finaux\n",
        "    print(f\"Total des réponses valides : {total}, Réponses correctes : {correct}\")\n",
        "\n",
        "    # Retourner la précision en pourcentage\n",
        "    return (correct / total) * 100\n",
        "\n",
        "# Utiliser le script\n",
        "file1 = 'qcm_output1.tsv'  # Fichier que vous avez créé\n",
        "file2 = '/content/drive/MyDrive/dataset/data/cti-mcq.tsv'  # Ancien fichier de données\n",
        "\n",
        "# Comparer la colonne 'Correct Answer' du fichier qcm_output.tsv avec la colonne 'GT' du fichier cti-mcq.tsv\n",
        "print('Précision:', compare_datasets(file1, file2, 'Correct Answer', 'GT'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q96PIWhuVqdJ",
        "outputId": "1fbad073-c09b-42b5-d75e-15794ee679b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Réponse invalide à la ligne 24: NAN\n",
            "Réponse invalide à la ligne 25: NAN\n",
            "Réponse invalide à la ligne 42: A, B, D\n",
            "Réponse invalide à la ligne 43: NAN\n",
            "Réponse invalide à la ligne 45: A, B\n",
            "Réponse invalide à la ligne 52: NAN\n",
            "Réponse invalide à la ligne 53: NAN\n",
            "Réponse invalide à la ligne 54: NAN\n",
            "Réponse invalide à la ligne 55: NAN\n",
            "Réponse invalide à la ligne 56: NAN\n",
            "Réponse invalide à la ligne 90: NAN\n",
            "Réponse invalide à la ligne 130: NAN\n",
            "Réponse invalide à la ligne 131: NAN\n",
            "Réponse invalide à la ligne 148: A, B, D\n",
            "Réponse invalide à la ligne 149: NAN\n",
            "Réponse invalide à la ligne 151: A, B\n",
            "Réponse invalide à la ligne 158: NAN\n",
            "Réponse invalide à la ligne 159: NAN\n",
            "Réponse invalide à la ligne 160: NAN\n",
            "Réponse invalide à la ligne 161: NAN\n",
            "Réponse invalide à la ligne 162: NAN\n",
            "Réponse invalide à la ligne 196: NAN\n",
            "Réponse invalide à la ligne 236: NAN\n",
            "Réponse invalide à la ligne 237: NAN\n",
            "Réponse invalide à la ligne 254: A, B, D\n",
            "Réponse invalide à la ligne 255: NAN\n",
            "Réponse invalide à la ligne 257: A, B\n",
            "Réponse invalide à la ligne 264: NAN\n",
            "Réponse invalide à la ligne 265: NAN\n",
            "Réponse invalide à la ligne 266: NAN\n",
            "Réponse invalide à la ligne 267: NAN\n",
            "Réponse invalide à la ligne 268: NAN\n",
            "Réponse invalide à la ligne 302: NAN\n",
            "Total des réponses valides : 285, Réponses correctes : 73\n",
            "Précision: 25.6140350877193\n"
          ]
        }
      ]
    }
  ]
}